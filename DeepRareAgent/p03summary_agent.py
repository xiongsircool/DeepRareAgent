# -*- coding: utf-8 -*-
"""
æ±‡æ€»è¯Šæ–­æ™ºèƒ½ä½“ (P03)

æ•´åˆå¤šä¸ªä¸“å®¶ç»„çš„è¯Šæ–­æŠ¥å‘Šï¼Œç”Ÿæˆç¬¦åˆä¸´åºŠè§„èŒƒçš„ç»¼åˆè¯Šæ–­æŠ¥å‘Šã€‚
"""

from typing import Any, Dict
from pathlib import Path

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables import RunnableConfig

from DeepRareAgent.config import settings
from DeepRareAgent.schema import MainGraphState
from DeepRareAgent.utils.model_factory import create_llm_from_config
from DeepRareAgent.utils.report_utils import process_expert_report_references


def _load_system_prompt() -> str:
    """
    åŠ è½½ç³»ç»Ÿæç¤ºè¯
    
    Returns:
        ç³»ç»Ÿæç¤ºè¯å†…å®¹
        
    Raises:
        FileNotFoundError: å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨
    """
    prompt_path = settings.summary_agent.system_prompt_path
    
    if not Path(prompt_path).exists():
        raise FileNotFoundError(
            f"æ±‡æ€»æ™ºèƒ½ä½“æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}\n"
            f"è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ summary_agent.system_prompt_path"
        )
    
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()


def _format_expert_reports(
    reports: Dict[str, str],
    expert_pool: Dict[str, Any]
) -> str:
    """
    æ ¼å¼åŒ–ä¸“å®¶æŠ¥å‘Šï¼Œå¤„ç†å¼•ç”¨å¹¶æ•´åˆ
    
    Args:
        reports: ä¸“å®¶ç»„æŠ¥å‘Šå­—å…¸ {group_id: report}
        expert_pool: ä¸“å®¶æ± ï¼ŒåŒ…å«è¯æ®åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–åçš„æ‰€æœ‰æŠ¥å‘Šæ–‡æœ¬
    """
    formatted_reports = []
    
    for group_id, report in reports.items():
        # å¤„ç†è¯æ®å¼•ç”¨
        expert_data = expert_pool.get(group_id, {})
        evidences = expert_data.get("evidences", [])
        
        if evidences:
            report = process_expert_report_references(report, evidences)
        
        # æ ¼å¼åŒ–å•ä¸ªæŠ¥å‘Š
        formatted_report = f"""
{'=' * 60}
ä¸“å®¶ç»„: {group_id}
{'=' * 60}

{report}
"""
        formatted_reports.append(formatted_report)
    
    return "\n".join(formatted_reports)


def summary_node(state: MainGraphState, config: RunnableConfig) -> Dict[str, Any]:
    """
    æ±‡æ€»èŠ‚ç‚¹ï¼šæ•´åˆå¤šä½ä¸“å®¶çš„è¯Šæ–­æŠ¥å‘Šï¼Œç”Ÿæˆæœ€ç»ˆç»¼åˆè¯Šæ–­
    
    Args:
        state: ä¸»å›¾çŠ¶æ€ï¼ŒåŒ…å«ä¸“å®¶ç»„æŠ¥å‘Š
        config: è¿è¡Œé…ç½®
        
    Returns:
        åŒ…å«æœ€ç»ˆæŠ¥å‘Šçš„çŠ¶æ€æ›´æ–°
        
    Raises:
        ValueError: å¦‚æœæ²¡æœ‰ä¸“å®¶æŠ¥å‘Š
        FileNotFoundError: å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨
        Exception: LLM è°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
    """
    print("\n" + "=" * 80)
    print(">>> [æ±‡æ€»èŠ‚ç‚¹] å¼€å§‹æ•´åˆä¸“å®¶è¯Šæ–­æŠ¥å‘Š")
    print("=" * 80)
    
    # 1. éªŒè¯è¾“å…¥
    blackboard = state.get("blackboard", {})
    reports = blackboard.get("published_reports", {})
    
    if not reports:
        error_msg = "é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•ä¸“å®¶æŠ¥å‘Šï¼Œæ— æ³•ç”Ÿæˆç»¼åˆè¯Šæ–­"
        print(f"[ERROR] {error_msg}")
        raise ValueError(error_msg)
    
    print(f"\n[INFO] ä¸“å®¶ç»„æ•°é‡: {len(reports)}")
    for group_id in reports.keys():
        print(f"   - {group_id}")
    
    # 2. æ ¼å¼åŒ–ä¸“å®¶æŠ¥å‘Šå¹¶æ”¶é›†æ‰€æœ‰è¯æ®
    print("\n[NOTE] æ•´åˆä¸“å®¶æŠ¥å‘Š...")
    expert_pool = state.get("expert_pool", {})
    all_reports_text = _format_expert_reports(reports, expert_pool)
    
    # æ”¶é›†æ‰€æœ‰è¯æ®ï¼Œå½¢æˆç»Ÿä¸€çš„è¯æ®æ± ä¾›æ±‡æ€»æŠ¥å‘Šä½¿ç”¨
    all_evidences = []
    for group_id in sorted(reports.keys()):  # æ’åºç¡®ä¿é¡ºåºä¸€è‡´
        expert_data = expert_pool.get(group_id, {})
        evidences = expert_data.get("evidences", [])
        all_evidences.extend(evidences)
    
    print(f"ğŸ“š æ”¶é›†åˆ° {len(all_evidences)} æ¡è¯æ®ä¾›æ±‡æ€»å¼•ç”¨")
    
    # 3. åŠ è½½ç³»ç»Ÿæç¤ºè¯
    print("ğŸ“– åŠ è½½ç³»ç»Ÿæç¤ºè¯...")
    system_prompt = _load_system_prompt()
    
    # 4. æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ ¼å¼ï¼‰
    patient_portrait = state.get("patient_portrait", "")
    summary_style = state.get("summary_style", "")
    
    # å¦‚æœæœ‰è‡ªå®šä¹‰æ ¼å¼è¦æ±‚ï¼Œä½¿ç”¨è‡ªå®šä¹‰ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤
    if summary_style:
        # ç”¨æˆ·è‡ªå®šä¹‰æŠ¥å‘Šæ ¼å¼
        format_instruction = f"""
è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¦æ±‚ç”ŸæˆæŠ¥å‘Šï¼š

{summary_style}
"""
    else:
        # é»˜è®¤æ ¼å¼è¦æ±‚
        format_instruction = """
è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºè¯çš„æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥ç”Ÿæˆä¸€ä»½ä¸´åºŠè¯Šæ–­æŠ¥å‘Šã€‚

è¦æ±‚ï¼š
- æ˜ç¡®ç»™å‡ºè¯Šæ–­ç»“è®ºï¼Œä¸è¦æè¿°ä¸“å®¶è®¨è®ºè¿‡ç¨‹
- æä¾›å…·ä½“çš„æ£€æŸ¥å’Œæ²»ç–—å»ºè®®
- åŒ…å«å®ç”¨çš„éšè®¿è®¡åˆ’å’Œæ³¨æ„äº‹é¡¹
- ä½¿ç”¨æ‚£è€…å’ŒåŒ»ç”Ÿéƒ½èƒ½ç†è§£çš„ä¸“ä¸šè¯­è¨€
- å¯åœ¨å…³é”®è¯Šæ–­ä¾æ®å¤„ä½¿ç”¨ <ref>N</ref> å¼•ç”¨ä¸“å®¶æŠ¥å‘Šä¸­çš„è¯æ®ï¼Œå¢å¼ºå¯è¿½æº¯æ€§
"""
    
    user_prompt = f"""ä»¥ä¸‹æ˜¯æ‚£è€…çš„ä¸´åºŠä¿¡æ¯å’Œå¤šä½ä¸“å®¶çš„è¯Šæ–­åˆ†æï¼Œè¯·ä¸ºæ‚£è€…å‡ºå…·æ­£å¼çš„ç½•è§ç—…è¯Šæ–­æŠ¥å‘Šã€‚

{'ã€æ‚£è€…ä¿¡æ¯ã€‘' + chr(10) + patient_portrait + chr(10) if patient_portrait else ''}
ã€ä¸“å®¶è¯Šæ–­åˆ†æã€‘
{all_reports_text}
{format_instruction}
"""
    
    # 5. è°ƒç”¨ LLM ç”ŸæˆæŠ¥å‘Š
    print("[LLM] è°ƒç”¨ LLM ç”Ÿæˆç»¼åˆè¯Šæ–­æŠ¥å‘Š...")
    
    llm = create_llm_from_config(settings.summary_agent)
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    response = llm.invoke(messages)
    final_report = response.content
    
    # å¤„ç†æ±‡æ€»æŠ¥å‘Šä¸­çš„è¯æ®å¼•ç”¨ï¼ˆå¦‚æœLLMä½¿ç”¨äº† <ref> æ ‡ç­¾ï¼‰
    if all_evidences:
        final_report = process_expert_report_references(final_report, all_evidences)
    
    print(f"\n[SUCCESS] ç»¼åˆæŠ¥å‘Šç”ŸæˆæˆåŠŸï¼ˆ{len(final_report)} å­—ç¬¦ï¼‰")
    print("=" * 80 + "\n")
    
    # 6. è¿”å›ç»“æœ
    return {
        "messages": [AIMessage(content=final_report)],
        "final_report": final_report
    }


# å¯¼å‡º
__all__ = ["summary_node"]


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    """æµ‹è¯•æ±‡æ€»èŠ‚ç‚¹"""
    from DeepRareAgent.schema import SharedBlackboard

    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_state = {
        "patient_portrait": "35å²ç”·æ€§ï¼Œå››è‚¢ç–¼ç—›10å¹´ï¼Œçš®è‚¤è¡€ç®¡è§’è´¨ç˜¤5å¹´ï¼Œå®¶æ—å²æ¯äº²è‚¾è¡°ç«­",
        "blackboard": {
            "published_reports": {
                "group_1": """# è¯Šæ–­æŠ¥å‘Š - ä¸“å®¶ç»„1

## è¯Šæ–­æ„è§
æ³•å¸ƒé›·ç—…ï¼ˆFabry Diseaseï¼‰- é«˜åº¦æ€€ç–‘<ref>1</ref>

## ä¸»è¦ä¾æ®
1. å››è‚¢é˜µå‘æ€§ç–¼ç—›ï¼ˆå…¸å‹ Fabry å±è±¡ï¼‰
2. å°‘æ±—ç—‡ï¼ˆè‡ªä¸»ç¥ç»å—ç´¯ï¼‰
3. çš®è‚¤è¡€ç®¡è§’è´¨ç˜¤ï¼ˆç‰¹å¾æ€§è¡¨ç°ï¼‰<ref>2</ref>
4. å®¶æ—å²ç¬¦åˆ X-è¿é”é—ä¼ 

## å»ºè®®æ£€æŸ¥
- Î±-åŠä¹³ç³–è‹·é…¶Aæ´»æ€§æ£€æµ‹
- GLAåŸºå› æµ‹åº
""",
                "group_2": """# è¯Šæ–­æŠ¥å‘Š - ä¸“å®¶ç»„2

## è¯Šæ–­æ„è§
æ³•å¸ƒé›·ç—…ï¼ˆFabry Diseaseï¼‰- é«˜åº¦å¯èƒ½

## è¡¥å……è¯æ®
1. è‚¾åŠŸèƒ½å¼‚å¸¸ï¼ˆè‚Œé…å‡é«˜ï¼‰
2. å¿ƒè„è¶…å£°å¼‚å¸¸ï¼ˆå·¦å®¤å¢åšï¼‰
3. å¬åŠ›ä¸‹é™ï¼ˆæ„ŸéŸ³ç¥ç»æ€§ï¼‰

## é‰´åˆ«è¯Šæ–­
éœ€æ’é™¤å…¶ä»–æº¶é…¶ä½“è´®ç§¯ç—‡
"""
            }
        },
        "patient_info": {},
        "summary_with_dialogue": "",
        "expert_pool": {
            "group_1": {
                "evidences": [
                    "æ‚£è€…è‡ªè¿°æ‰‹è„šç–¼ç—›ï¼Œåƒçƒ§ç¼ä¸€æ ·ï¼Œå°¤å…¶å¤å¤©ä¸¥é‡ã€‚",
                    "ä½“æ£€å‘ç°èº¯å¹²éƒ¨ä½æœ‰çº¢è‰²å°ç‚¹ï¼Œå‹ä¹‹ä¸é€€è‰²ã€‚"
                ]
            },
            "group_2": {
                "evidences": []
            }
        },
        "round_count": 2,
        "max_rounds": 3
    }

    print("\n" + "=" * 80)
    print("æµ‹è¯•æ±‡æ€»èŠ‚ç‚¹")
    print("=" * 80)

    result = summary_node(test_state, {})

    print("\nç”Ÿæˆçš„ç»¼åˆæŠ¥å‘Šï¼š")
    print("=" * 80)
    print(result.get("final_report", "æ— æŠ¥å‘Š"))
    print("=" * 80)
